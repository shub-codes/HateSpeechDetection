|   Split |   output_dim | max_len                         | Total trainable parameters   | lemmitization?   |   Epochs |   lstm dropout |   lstm_dense |   Dense |   dropout |   val_accuracy |   val_f1 |   val_precision |   val_recall |   val_loss | Unnamed: 15                                                                                    |
|--------:|-------------:|:--------------------------------|:-----------------------------|:-----------------|---------:|---------------:|-------------:|--------:|----------:|---------------:|---------:|----------------:|-------------:|-----------:|:-----------------------------------------------------------------------------------------------|
|    0.75 |          200 | maximum of all lenghts of tweet | 3,719,147                    | No               |       10 |            0.3 |           64 |     128 |      0.5  |         0.7736 |   0.7739 |          0.7739 |       0.7739 |     0.6633 | as tokens were too large, ~149 as compared to 200 output_dim, the model hits a platueau        |
|    0.75 |          200 | maximum of training tweet       | 3,719,147                    | No               |       10 |            0.3 |           64 |     128 |      0.5  |         0.8819 |   0.8807 |          0.8862 |       0.8755 |     0.5694 | Tokens~28, hence model achives peak accuracy                                                   |
|    0.75 |          200 | maximum of training tweet       | 3,719,147                    | Yes              |       10 |            0.3 |           64 |     128 |      0.5  |         0.8772 |   0.877  |          0.8803 |       0.8739 |     0.5707 | Even after lemmitization the model is performing the same                                      |
|    0.75 |          300 | maximum of training tweet       | 5,258,247                    | yes              |       10 |            0.3 |           64 |     128 |      0.5  |         0.8772 |   0.8721 |          0.8738 |       0.8704 |     0.7628 | creates an instance of overfitting ans val_f1 score peaked at 4th epoch and then reduced       |
|    0.75 |          200 | maximum of training tweet       | 3,622,747                    | yes              |       10 |            0.3 |          128 |     128 |      0.5  |         0.8793 |   0.8784 |          0.8829 |       0.8741 |     0.5584 | atlease avoid overfitting                                                                      |
|    0.75 |          200 | maximum of training tweet       | 3,513,947                    | yes              |       10 |            0.3 |           64 |     128 |      0.25 |         0.8733 |   0.8735 |          0.8754 |       0.8716 |     0.6804 | nan                                                                                            |
|    0.75 |          200 | maximum of training tweet       | 3,513,947                    | yes              |        5 |            0.3 |           64 |     128 |      0.25 |         0.892  |   0.8906 |          0.895  |       0.8864 |     0.398  | the results ar best in 5 epochs, after which overtraining begins                               |
|    0.65 |          200 | maximum of training tweet       | 3,244,547                    | yes              |       10 |            0.3 |           64 |     128 |      0.25 |         0.8733 |   0.8735 |          0.8754 |       0.8716 |     0.6804 | nan                                                                                            |
|    0.65 |          200 | maximum of training tweet       | 3,244,547                    | yes              |        5 |            0.3 |           64 |     128 |      0.25 |         0.8868 |   0.8878 |          0.8932 |       0.8826 |     0.414  | we can see that after 5 epochs, the data begins to overfit and hence leads to reduced f1_score |
|    0.65 |          400 | maximum of training tweet       | 6,641,831                    | yes              |       10 |            0.3 |          128 |     256 |      0.4  |         0.8759 |   0.8766 |          0.8779 |       0.8752 |     0.7918 | nan                                                                                            |
|    0.65 |          400 | maximum of training tweet       | 6,641,831                    | yes              |        5 |            0.3 |          128 |     256 |      0.4  |         0.8863 |   0.886  |          0.8885 |       0.8836 |     0.4418 | nan                                                                                            |
|    0.65 |          200 | maximum of training tweet       | 3,200,131                    | yes              |       10 |            0.3 |           32 |      64 |      0.5  |         0.8705 |   0.8716 |          0.873  |       0.8702 |     0.7152 | nan                                                                                            |
|    0.65 |          200 | maximum of training tweet       | 3,200,131                    | yes              |        5 |            0.3 |           32 |      64 |      0.5  |         0.8892 |   0.8885 |          0.8988 |       0.8787 |     0.4705 | nan                                                                                            |
|    0.8  |          200 | maximum of training tweet       | 3,654,947                    | yes              |       10 |            0.3 |           64 |     128 |      0.5  |         0.8725 |   0.8727 |          0.8764 |       0.8691 |     0.6102 | nan                                                                                            |
|    0.8  |          200 | maximum of training tweet       | 3,654,947                    | yes              |        5 |            0.3 |           64 |     128 |      0.5  |         0.8905 |   0.8909 |          0.8949 |       0.8872 |     0.4006 | nan                                                                                            |
