{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i6Hz4z113Wwde2D64tThwN4EFrI-wzpi",
      "authorship_tag": "ABX9TyPJD9oy2J00ekr7Mw575/x9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shub-codes/HateSpeechDetection/blob/main/Hate_speech_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/thedevastator/hate-speech-and-offensive-language-detection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aTZRIIpbYoDZ",
        "outputId": "3e70daaf-3de8-4ba8-924e-6909c6b8acf8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./hate-speech-and-offensive-language-detection\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 1) importing libraries</h1>\n",
        "<ol>\n",
        "<li> <h3>  pandas - analyse, clean,explore and manipulate the dataset .<br>\n",
        "<li>  <h3> re or regex - A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern.<br>\n",
        "<li> <h3> nltk- natural language transformer kit.<br>\n"
      ],
      "metadata": {
        "id": "Tbb4d1WZTMYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "h6Au-lriS3pX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #to read and write csv files\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords') # natural language toolkit\n",
        "import re #regular expressions\n",
        "from nltk.corpus import stopwords # to remove words like a, the,is etc. such words contribute almost nothing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 2) Importing and reading data"
      ],
      "metadata": {
        "id": "M-vQQyaUp162"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/hate-speech-and-offensive-language-detection/train.csv\")\n",
        "print(df.head(5))\n",
        "print(f\"df has {df.shape[0]}  rows and {df.shape[1]} cols\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGfPcG62ZTK5",
        "outputId": "a5fccf44-76e0-4c90-c812-4bcb0c691fdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
            "0      3                  0                         0              3      2   \n",
            "1      3                  0                         3              0      1   \n",
            "2      3                  0                         3              0      1   \n",
            "3      3                  0                         2              1      1   \n",
            "4      6                  0                         6              0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "df has 24783  rows and 6 cols\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 3) CLEANING THE DATA USING REGULAR EXPRESSIONS </h1>"
      ],
      "metadata": {
        "id": "NtgBip1qnTG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopWords= set(stopwords.words('english'))\n",
        "stopWords.add('RT') # to add the RT which means retweet\n",
        "#remove html entity\n",
        "def remove_entity(raw_text):\n",
        "  text_regex= r\"&(#\\d+|#x[a-fA-F0-9]+|[a-zA-Z]+);\"\n",
        "  updated_text=re.sub(text_regex,\"\",raw_text)\n",
        "  return updated_text\n",
        "# removes user tags\n",
        "def remove_user_tags(raw_text):\n",
        "  user_regex=r\"@\\w+\"\n",
        "  updated_text=re.sub(user_regex,\"\",raw_text)\n",
        "  return updated_text\n",
        "# removes urls\n",
        "def remove_urls(raw_text):\n",
        "  url_regex=r\"https?://\\S+|www\\.\\S+\"\n",
        "  updated_text=re.sub(url_regex,\"\",raw_text,flags=re.IGNORECASE)\n",
        "  return updated_text\n",
        "# to remove unnecessary markings\n",
        "def remove_noise(raw_text):\n",
        "  noise_regex=r\"['\\\"!`]|\\.{2,}\"\n",
        "  updated_text=re.sub(noise_regex,\"\",raw_text)\n",
        "  return updated_text\n",
        "# to tokenise the string and remove stopwords\n",
        "def remove_stopwords(raw_text):\n",
        "  tokenize= nltk.word_tokenize(raw_text)\n",
        "  updated_text=[word for word in tokenize if word not in stopWords]\n",
        "  return \" \".join(updated_text)\n",
        "# calling all the above functions\n",
        "def preprocess(raw_text):\n",
        "  updated=[]\n",
        "  updated=[remove_entity(text) for text in raw_text]\n",
        "  updated=[remove_user_tags(text) for text in updated]\n",
        "  updated=[remove_urls(text) for text in updated]\n",
        "  updated=[remove_noise(text) for text in updated]\n",
        "  updated=[remove_stopwords(text) for text in updated]\n",
        "  return updated\n",
        "#clean_tweet=preprocess(tweet)"
      ],
      "metadata": {
        "id": "jxYRSC0ObSqq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Explaination of regex given above</h2>\n",
        "<ul>\n",
        "<li> <h3> HTML func regex = r\"&(#\\d+|#x[a-fA-F0-9]+|[a-zA-Z]+)\n",
        "  <ul> <li>r stands to tell that the text is raw\n",
        "      <li>  #\\d tells for numeric references eg &#12345\n",
        "      <li> #x[a-fA-F0-9] tells for hexadecimal numeric references\n",
        "      <li> [a-zA-Z] mathces named entities &amp, &lt\n",
        "  </ul>\n",
        "<li><h3> user tags func = r\"@\\w+\"\n",
        "  <ul> <li> \\w stands for  all alphanumeric letters and _\n",
        "        <li> example \"Hey @john_doe, check this out! Also, @alice123 and @user.test are here.\" changes to \"Hey user, check this out! Also, user and user are here.\"\n",
        "  </ul>\n",
        "<li><h3> url func = r\"https?://\\S+|www\\.\\S+\"\n",
        "  <ul> <li> https? means the s in last is optional. hence it mathces to both http and https.\n",
        "      <li> \\S+ matches any additional string\n",
        "      <li> www\\.\\S+ matches for all url in for www.dsfdjskn.com\n",
        "      <li> flag=re.IGNORECASE means it matches irrespective of case of letters\n",
        "  </ul>\n",
        "<li> <h3> remove noise func\n",
        "  <ul> <li> removes \", ', !' or two and more dots\n",
        "  </ul>\n"
      ],
      "metadata": {
        "id": "W-d-iVVxcwSJ"
      }
    }
  ]
}